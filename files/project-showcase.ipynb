{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset creation phase, I tested out different things, I tried to use the GPT-3.5-turbo API to create the [dataset](https://gist.github.com/Hero2323/bff12400cec5ab54467ea35ba89e976f) for me (didn't work), I tried using [scancodes](https://gist.github.com/Hero2323/da410d4f06547ef3b4bdb626bbde868b), and at the end I resorted to manual, model assisted labeling. Mainly, I used the [Fossology API](https://gist.github.com/Hero2323/7ed99af2e336216860ad74e6002de5db) to extract what it thinks are copyrights then using [google sheets](https://docs.google.com/spreadsheets/d/132NnbJT4nqb-hxPX-XRFvUWTUg9SW0-ueW2YkpykgSk/edit?usp=sharing) to more easily label the dataset as I could use conditional formatting for better visibility and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False Positive Detection Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this phase, I wrote various functions that simplified the iterative process of testing various models, vectorization methods, parameters, etc. As well as improved the formatting of results among other things. I tested out many models, including RF, SVM, NB, Bert, RNN, as well as many vectorization and embedding methods such as TF-IDF, BoW, GloVe, FastText, and BERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When testing many datasets at the same time, this function\n",
    "# can aggregate the results in cleaner manner\n",
    "\n",
    "def aggregate_reports(reports, print_aggregates=True):\n",
    "    dfs = []\n",
    "    for metric in ['precision', 'recall', 'f1-score']:\n",
    "        scores = []\n",
    "        for report in reports:\n",
    "            scores.append([report['0'][metric], report['1'][metric]])\n",
    "        scores = np.array(scores)\n",
    "        scores = scores[:, :2]\n",
    "        mean_scores = np.mean(scores, axis=0)\n",
    "        mean_scores = [f\"{score:.6f}\" for score in mean_scores]\n",
    "        df = pd.DataFrame(scores, columns=['0', '1'])\n",
    "        df.loc['Mean'] = mean_scores\n",
    "        df['Metric'] = metric\n",
    "        dfs.append(df)\n",
    "    if print_aggregates:\n",
    "        print(\"## Precision\")\n",
    "        print(dfs[0].to_markdown())\n",
    "        print(\"## Recall\")\n",
    "        print(dfs[1].to_markdown())\n",
    "        print(\"## F1-score\")\n",
    "        print(dfs[2].to_markdown())\n",
    "    else:\n",
    "        return dfs[0], dfs[1], dfs[2]\n",
    "\n",
    "# Below is what the output looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "|      |        0 |        1 | Metric    |\n",
    "|:-----|---------:|---------:|:----------|\n",
    "| 0    | 0.992318 | 0.972816 | precision |\n",
    "| 1    | 0.977273 | 0.679558 | precision |\n",
    "| 2    | 0.837209 | 0.783333 | precision |\n",
    "| 3    | 0.966989 | 0.923404 | precision |\n",
    "| 4    | 0.992459 | 0.973096 | precision |\n",
    "| Mean | 0.95325  | 0.866441 | precision |\n",
    "## Recall\n",
    "|      |        0 |        1 | Metric   |\n",
    "|:-----|---------:|---------:|:---------|\n",
    "| 0    | 0.990244 | 0.978516 | recall   |\n",
    "| 1    | 0.869663 | 0.931818 | recall   |\n",
    "| 2    | 0.80597  | 0.817391 | recall   |\n",
    "| 3    | 0.985234 | 0.841085 | recall   |\n",
    "| 4    | 0.990422 | 0.978738 | recall   |\n",
    "| Mean | 0.928307 | 0.90951  | recall   |\n",
    "## F1-score\n",
    "|      |        0 |        1 | Metric   |\n",
    "|:-----|---------:|---------:|:---------|\n",
    "| 0    | 0.99128  | 0.975657 | f1-score |\n",
    "| 1    | 0.920333 | 0.785942 | f1-score |\n",
    "| 2    | 0.821293 | 0.8      | f1-score |\n",
    "| 3    | 0.976026 | 0.880325 | f1-score |\n",
    "| 4    | 0.99144  | 0.975909 | f1-score |\n",
    "| Mean | 0.940074 | 0.883567 | f1-score |\n",
    "None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easily shows me exactly which rows were misclassified by the model\n",
    "def get_missclassified_rows(X, y_true, y_pred, only_this_class = [0, 1], return_index=False):\n",
    "    if type(y_true) != list:\n",
    "        y_true = y_true.tolist()\n",
    "    if type(y_pred) != list:\n",
    "        y_pred = y_pred.tolist()\n",
    "    if type(X) != list:\n",
    "        X = X.tolist()\n",
    "    missclassified_rows = []\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] != y_pred[i] and y_true[i] in only_this_class:\n",
    "            missclassified_rows.append(i)\n",
    "    if return_index:\n",
    "        return [(y_pred[i], i, X[i]) for i in missclassified_rows]\n",
    "    else:\n",
    "        return [(y_pred[i], X[i]) for i in missclassified_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the main function I used for training the model, in this I can easily include\n",
    "# any number of parameters for the preprocessing function, which at the time could have\n",
    "# different number of parameters as I worked on developing it\n",
    "# Additionally, I transitioned to a different way of visualizing the results here were I test\n",
    "# on all the datasets (including the training one) and simply view what percent of the \n",
    "# rows were missclasisfied\n",
    "\n",
    "def train(svm, vectorizer, threshold, preprocess_function,**kwargs):\n",
    "    X_train_tfidf = vectorizer.fit_transform(preprocess_function(X_train, **kwargs))\n",
    "    X_test_tfidf = vectorizer.transform(preprocess_function(X_test, **kwargs))\n",
    "    X_1_tfidf = vectorizer.transform(preprocess_function(X_1, **kwargs))\n",
    "    X_2_tfidf = vectorizer.transform(preprocess_function(X_2, **kwargs))\n",
    "    X_3_tfidf = vectorizer.transform(preprocess_function(X_3, **kwargs))\n",
    "    preprocessed_X = preprocess_function(X, **kwargs)\n",
    "    X_tfidf = vectorizer.transform(preprocessed_X)\n",
    "    svm.fit(X_train_tfidf, y_train)\n",
    "    if True: #svm.probability:\n",
    "        y_pred = svm.predict_proba(X_test_tfidf)\n",
    "        y_pred_1 = svm.predict_proba(X_1_tfidf)\n",
    "        y_pred_2 = svm.predict_proba(X_2_tfidf)\n",
    "        y_pred_3 = svm.predict_proba(X_3_tfidf)\n",
    "        y_pred_4 = svm.predict_proba(X_tfidf)\n",
    "        if threshold is None:\n",
    "            y_pred_classification = np.argmax(y_pred, axis=1)\n",
    "            y_pred_1_classification = np.argmax(y_pred_1, axis=1)\n",
    "            y_pred_2_classification = np.argmax(y_pred_2, axis=1)\n",
    "            y_pred_3_classification = np.argmax(y_pred_3, axis=1)\n",
    "            y_pred_4_classification = np.argmax(y_pred_4, axis=1)\n",
    "        else:\n",
    "            y_pred_classification = [np.argmax(y) if max(y) > threshold else 0 for y in y_pred]\n",
    "            y_pred_1_classification = [np.argmax(y) if max(y) > threshold else 0 for y in y_pred_1]\n",
    "            y_pred_2_classification = [np.argmax(y) if max(y) > threshold else 0 for y in y_pred_2]\n",
    "            y_pred_3_classification = [np.argmax(y) if max(y) > threshold else 0 for y in y_pred_3]\n",
    "            y_pred_4_classification = [np.argmax(y) if max(y) > threshold else 0 for y in y_pred_4]\n",
    "    else:\n",
    "        y_pred_classification = svm.predict(X_test_tfidf)\n",
    "        y_pred_1_classification = svm.predict(X_1_tfidf)\n",
    "        y_pred_2_classification = svm.predict(X_2_tfidf)\n",
    "        y_pred_3_classification = svm.predict(X_3_tfidf)\n",
    "        y_pred_4_classification = svm.predict(X_tfidf)\n",
    "    report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "    report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "    report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "    report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "    report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "    miss_classified_rows_0 = get_missclassified_rows(preprocessed_X, y, y_pred_4_classification, only_this_class=[0], return_index=True)\n",
    "    miss_classified_rows_1 = get_missclassified_rows(preprocessed_X, y, y_pred_4_classification, only_this_class=[1], return_index=True)\n",
    "    #aggregate_reports([report, report_1, report_2, report_3, report_4])\n",
    "    print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "    print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "    \n",
    "# Here is how I would call it\n",
    "clf = OneVsRestClassifier(SVC(probability=True, C=25))\n",
    "test = train(clf, TfidfVectorizer(ngram_range=(1, 2), binary=True), None, preprocess_function,\n",
    "                                            replace_dates=True, remove_numbers=True, replace_copyright_symbols=True,\n",
    "                                            remove_whitespaces=True, lower=True, remove_special_characters=True, \n",
    "                                            replace_emails=True, replace_entities=True,\n",
    "                                            spacy_model=spacy.load('../NER_models/train-semi-supervised-dataset-1/model-best'))\n",
    "# and Here is what the output looks like \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of missclassifications in class 0:  25.0 out of a total sample of:  16377.0  - about  0.15 % of the class was missclassified\n",
    "\n",
    "Number of missclassifications in class 1:  43.0 out of a total sample of:  5393.0  - about  0.8 % of the class was missclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this phase, I used [doccano](https://github.com/doccano/doccano) for easier labeling. I tested out several models and settled on SpaCy's en_core_web_sm model for its lightweight, performance, and simple training. I wrote several functions that automatically convert between JSONL (needed by doccano), CSV (default data format I was using), and .spacy (binary format used by SpaCy for training). Below I showcase the utility functions I wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def text_to_json(sentences):\n",
    "    \"\"\"\n",
    "    Convert list of sentences to a specific JSON format.\n",
    "    \"\"\"\n",
    "    new_json = list()\n",
    "    for sentence in tqdm(sentences):  # Progress bar for iterating through sentences\n",
    "        labels = list()  # Initializing empty labels\n",
    "        # Appending the sentence and its (empty) labels to the resulting JSON\n",
    "        new_json.append({'text': sentence, \"labels\": labels})\n",
    "    return new_json\n",
    "\n",
    "def text_to_json_model_assisted(sentences, model):\n",
    "    \"\"\"\n",
    "    Convert list of sentences to a JSON format using a model for predictions.\n",
    "    \"\"\"\n",
    "    sentences = model(sentences)  # Using the model to process sentences\n",
    "    new_json = list()\n",
    "    for sentence in tqdm(sentences):  # Progress bar for iterating through sentences\n",
    "        labels = list()\n",
    "        for e in sentence.ents:  # Iterating through detected entities in the sentence\n",
    "            # Appending start, end character positions and label of the entity\n",
    "            labels.append([e.start_char, e.end_char, e.label_])\n",
    "        # Appending the sentence and its detected labels to the resulting JSON\n",
    "        new_json.append({'text': sentence.text, \"labels\": labels})\n",
    "    return new_json\n",
    "\n",
    "def text_to_json_labels_separate(sentences, labels, entity_name):\n",
    "    \"\"\"\n",
    "    Convert list of sentences and labels to a JSON format with specific entity name.\n",
    "    \"\"\"\n",
    "    new_json = list()\n",
    "    for sentence, label in tqdm(zip(sentences, labels)):  # Progress bar for iterating through sentence-label pairs\n",
    "        if label is None or label == '':\n",
    "            continue  # If label is empty or None, skip the iteration\n",
    "        \n",
    "        # Search for the exact match of the label in the sentence\n",
    "        pattern = r\"\\b\" + re.escape(label) + r\"\\b\"\n",
    "        match = re.search(pattern, sentence)\n",
    "        if not match:\n",
    "            continue  # If label is not found in the sentence, skip the iteration\n",
    "        \n",
    "        # Appending the sentence and the position of its label to the resulting JSON\n",
    "        new_json.append({'text': sentence, \"labels\": [match.start(), match.end(), entity_name]})\n",
    "    return new_json\n",
    "\n",
    "def write_json_to_disk(new_json, path):\n",
    "    \"\"\"\n",
    "    Write a list of dictionaries in the JSONL format to disk.\n",
    "    \"\"\"\n",
    "    with open(path, 'w') as f:\n",
    "        for item in new_json:\n",
    "            # Writing each dictionary in the list as a separate line in the JSONL file\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "def convert_jsonl_to_spacy(jsonl_path, spacy_path):\n",
    "    \"\"\"\n",
    "    Load data from a JSONL file and convert it to spaCy's training format.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            text = item['text']\n",
    "            # Extracting entities from the JSONL line\n",
    "            entities = [(e[0], e[1], e[2]) for e in item['labels']]\n",
    "            data.append((text, {'entities': entities}))\n",
    "    \n",
    "    nlp = spacy.blank('en')  # Creating a blank English NLP object\n",
    "    doc_bin = spacy.tokens.DocBin()  # Initializing a DocBin for efficient storage of `Doc` objects\n",
    "    for text, annotations in data:\n",
    "        doc = nlp.make_doc(text)  # Creating a `Doc` object from text\n",
    "        example = spacy.training.Example.from_dict(doc, annotations)  # Creating an example from the `Doc` and its annotations\n",
    "        doc_bin.add(example.reference)  # Adding the `Doc` to the DocBin\n",
    "    doc_bin.to_disk(spacy_path)  # Saving the DocBin to disk\n",
    "\n",
    "def spacy_train_test_split(file_path, split=0.2, random_state=42, shuffle=True):\n",
    "    \"\"\"\n",
    "    Split spaCy formatted data into training and testing sets.\n",
    "    \"\"\"\n",
    "    doc_bin = spacy.tokens.DocBin().from_disk(file_path)  # Loading the DocBin from disk\n",
    "    nlp = spacy.load(\"en_core_web_sm\")  # Loading the English small core model\n",
    "    docs = list(doc_bin.get_docs(nlp.vocab))  # Retrieving `Doc` objects from the DocBin using the model's vocabulary\n",
    "    # Splitting the `Doc` objects into training and testing sets\n",
    "    train_docs, test_docs = train_test_split(docs, test_size=split, random_state=random_state, shuffle=shuffle)\n",
    "    train_doc_bin = spacy.tokens.DocBin(docs=train_docs)  # Creating a DocBin for training docs\n",
    "    test_doc_bin = spacy.tokens.DocBin(docs=test_docs)  # Creating a DocBin for testing docs\n",
    "    \n",
    "    # Deriving the paths for saving the training and testing DocBins\n",
    "    train_path = file_path.split('.spacy')[0] + '-train.spacy'\n",
    "    test_path = file_path.split('.spacy')[0] + '-test.spacy'\n",
    "\n",
    "    train_doc_bin.to_disk(train_path)  # Saving the training DocBin to disk\n",
    "    test_doc_bin.to_disk(test_path)  # Saving the testing DocBin to disk\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
